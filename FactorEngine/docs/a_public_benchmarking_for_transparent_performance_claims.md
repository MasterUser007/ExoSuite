# A. Public Benchmarking for Transparent Performance Claims

PrimeEngineAI publishes public benchmarks under controlled, reproducible conditions. These benchmarks:
• Use standardized candidate datasets stratified across digit classes
• Employ consistent runtime environments across all peer comparisons
• Include performance indicators such as:
   - Candidate throughput (candidates processed per second)
   - Latency percentiles by pipeline stage (p50, p95, p99)
   - False positive and false negative rates
   - Efficiency of symbolic and probabilistic filters by numeric region

All results are published alongside input hashes, system configuration metadata, and executable test scripts to ensure open reproducibility.

